{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a39oChDvxDdG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6a6HUgxqbGZ",
        "outputId": "940af1dc-eb85-4975-d5b0-eecff6e287e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TensorRT-LLM'...\n",
            "remote: Enumerating objects: 16411, done.\u001b[K\n",
            "remote: Counting objects: 100% (8515/8515), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1962/1962), done.\u001b[K\n",
            "remote: Total 16411 (delta 6783), reused 7593 (delta 6493), pack-reused 7896\u001b[K\n",
            "Receiving objects: 100% (16411/16411), 266.68 MiB | 24.61 MiB/s, done.\n",
            "Resolving deltas: 100% (11953/11953), done.\n",
            "Note: switching to '5955b8afbad2ddcc3156202b16c567e94c52248f'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "Updating files: 100% (1741/1741), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b v0.8.0 https://github.com/NVIDIA/TensorRT-LLM.git\n",
        "!cd TensorRT-LLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lndMC2Rsg26o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjK-tf1NtXdI",
        "outputId": "577ad741-70dc-4894-984f-ecc06f4432a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmZT-jeWrAUX",
        "outputId": "35ceb923-2770-40d7-a673-fee112de27a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "4pxMblmdrd3q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "#!git remote set-url origin https://{deepika1703}:{12345678}@github.com/{deepika1703}/project.git\n",
        "!git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEg6plOHuZfk",
        "outputId": "c3497633-49ae-4cc2-8562-7d55d5006ed5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'Meta-Llama-3-8B-Instruct'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 63 (delta 21), reused 11 (delta 11), pack-reused 31 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (63/63), 4.50 MiB | 5.28 MiB/s, done.\n",
            "Filtering content: 100% (6/6), 5.91 GiB | 12.25 MiB/s, done.\n",
            "Encountered 4 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel-00003-of-00004.safetensors\n",
            "\tmodel-00001-of-00004.safetensors\n",
            "\tmodel-00002-of-00004.safetensors\n",
            "\toriginal/consolidated.00.pth\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Drengskapur\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "# @title {display-mode:\"form\"}\n",
        "# @markdown <br/><br/><center><img src=\"https://cdn.jsdelivr.net/gh/drengskapur/docker-in-colab/assets/docker.svg\" height=\"150\"><img src=\"https://cdn.jsdelivr.net/gh/drengskapur/docker-in-colab/assets/colab.svg\" height=\"150\"></center><br/>\n",
        "# @markdown <center><h1>Docker in Colab</h1></center><center>github.com/drengskapur/docker-in-colab<br/><br/><br/><b>udocker(\"run hello-world\")</b></center><br/>\n",
        "def udocker_init():\n",
        "    import os\n",
        "    if not os.path.exists(\"/home/user\"):\n",
        "        !pip install udocker > /dev/null\n",
        "        !udocker --allow-root install > /dev/null\n",
        "        !useradd -m user > /dev/null\n",
        "    print(f'Docker-in-Colab 1.1.0\\n')\n",
        "    print(f'Usage:     udocker(\"--help\")')\n",
        "    print(f'Examples:  https://github.com/indigo-dc/udocker?tab=readme-ov-file#examples')\n",
        "\n",
        "    def execute(command: str):\n",
        "        user_prompt = \"\\033[1;32muser@pc\\033[0m\"\n",
        "        print(f\"{user_prompt}$ udocker {command}\")\n",
        "        !su - user -c \"udocker $command\"\n",
        "\n",
        "    return execute\n",
        "\n",
        "udocker = udocker_init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGX_b5JZWy0M",
        "outputId": "305f57f5-02a2-4112-cf6c-512d5f4bb871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Docker-in-Colab 1.1.0\n",
            "\n",
            "Usage:     udocker(\"--help\")\n",
            "Examples:  https://github.com/indigo-dc/udocker?tab=readme-ov-file#examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain and start the basic docker image environment.\n",
        "!docker run --rm --runtime=nvidia --gpus all --volume ${PWD}:/TensorRT-LLM --entrypoint /bin/bash -it --workdir /TensorRT-LLM nvidia/cuda:12.1.0-devel-ubuntu22.04\n",
        "\n",
        "# Install dependencies, TensorRT-LLM requires Python 3.10\n",
        "!apt-get update && apt-get -y install python3.10 python3-pip openmpi-bin libopenmpi-dev\n",
        "\n",
        "# Install the stable version (corresponding to the cloned branch) of TensorRT-LLM.\n",
        "!pip3 install tensorrt_llm==0.8.0 -U --extra-index-url https://pypi.nvidia.com\n",
        "#!/tmp/llama/7B/trt_engines/bf16/1-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqLaQnqiazhX",
        "outputId": "cc4c2f7d-becf-4aa7-836a-17b31434cd72"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: docker: command not found\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "openmpi-bin is already the newest version (4.1.2-2ubuntu1).\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.3).\n",
            "python3-pip is already the newest version (22.0.2+dfsg-1ubuntu0.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Collecting tensorrt_llm==0.8.0\n",
            "  Using cached https://pypi.nvidia.com/tensorrt-llm/tensorrt_llm-0.8.0-cp310-cp310-linux_x86_64.whl (1126.4 MB)\n",
            "Collecting accelerate==0.25.0 (from tensorrt_llm==0.8.0)\n",
            "  Using cached accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "Requirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm==0.8.0) (1.2.1)\n",
            "Collecting colored (from tensorrt_llm==0.8.0)\n",
            "  Using cached colored-2.2.4-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: cuda-python in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm==0.8.0) (12.2.1)\n",
            "Collecting diffusers==0.15.0 (from tensorrt_llm==0.8.0)\n",
            "  Using cached diffusers-0.15.0-py3-none-any.whl (851 kB)\n",
            "Collecting lark (from tensorrt_llm==0.8.0)\n",
            "  Using cached lark-1.1.9-py3-none-any.whl (111 kB)\n",
            "Collecting mpi4py (from tensorrt_llm==0.8.0)\n",
            "  Using cached mpi4py-3.1.6.tar.gz (2.4 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm==0.8.0) (1.25.2)\n",
            "Collecting onnx>=1.12.0 (from tensorrt_llm==0.8.0)\n",
            "  Using cached onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "Collecting polygraphy (from tensorrt_llm==0.8.0)\n",
            "  Using cached https://pypi.nvidia.com/polygraphy/polygraphy-0.49.9-py2.py3-none-any.whl (346 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm==0.8.0) (5.9.5)\n",
            "Collecting pynvml>=11.5.0 (from tensorrt_llm==0.8.0)\n",
            "  Using cached pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm==0.8.0) (0.1.99)\n",
            "Collecting tensorrt==9.2.0.post12.dev5 (from tensorrt_llm==0.8.0)\n",
            "  Using cached https://pypi.nvidia.com/tensorrt/tensorrt-9.2.0.post12.dev5.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch<=2.2.0a (from tensorrt_llm==0.8.0)\n",
            "  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "Collecting transformers==4.36.1 (from tensorrt_llm==0.8.0)\n",
            "  Using cached transformers-4.36.1-py3-none-any.whl (8.3 MB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorrt_llm==0.8.0) (0.43.0)\n",
            "Collecting optimum (from tensorrt_llm==0.8.0)\n",
            "  Using cached optimum-1.19.2-py3-none-any.whl (417 kB)\n",
            "Collecting evaluate (from tensorrt_llm==0.8.0)\n",
            "  Using cached evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "Collecting janus (from tensorrt_llm==0.8.0)\n",
            "  Using cached janus-1.0.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting nvidia-ammo~=0.7.0 (from tensorrt_llm==0.8.0)\n",
            "  Using cached https://pypi.nvidia.com/nvidia-ammo/nvidia_ammo-0.7.4-cp310-cp310-linux_x86_64.whl (975 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.25.0->tensorrt_llm==0.8.0) (24.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.25.0->tensorrt_llm==0.8.0) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.25.0->tensorrt_llm==0.8.0) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.25.0->tensorrt_llm==0.8.0) (0.4.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm==0.8.0) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm==0.8.0) (3.14.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm==0.8.0) (7.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm==0.8.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.15.0->tensorrt_llm==0.8.0) (2.31.0)\n",
            "Collecting tensorrt_libs==9.2.0.post12.dev5 (from tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/tensorrt-libs/tensorrt_libs-9.2.0.post12.dev5-py2.py3-none-manylinux_2_17_x86_64.whl (1076.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 GB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorrt_bindings==9.2.0.post12.dev5 (from tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/tensorrt-bindings/tensorrt_bindings-9.2.0.post12.dev5-cp310-none-manylinux_2_17_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.19,>=0.14 (from transformers==4.36.1->tensorrt_llm==0.8.0)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.1->tensorrt_llm==0.8.0) (4.66.4)\n",
            "Collecting nvidia-cuda-runtime-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.1.17-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cublas-cu12/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from nvidia-ammo~=0.7.0->tensorrt_llm==0.8.0)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from nvidia-ammo~=0.7.0->tensorrt_llm==0.8.0) (3.3)\n",
            "Collecting onnxruntime~=1.16.1 (from nvidia-ammo~=0.7.0->tensorrt_llm==0.8.0)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx-graphsurgeon (from nvidia-ammo~=0.7.0->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/onnx-graphsurgeon/onnx_graphsurgeon-0.5.2-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from nvidia-ammo~=0.7.0->tensorrt_llm==0.8.0) (1.11.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0->tensorrt_llm==0.8.0) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<=2.2.0a->tensorrt_llm==0.8.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<=2.2.0a->tensorrt_llm==0.8.0) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<=2.2.0a->tensorrt_llm==0.8.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<=2.2.0a->tensorrt_llm==0.8.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cudnn-cu12/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12 (from tensorrt_libs==9.2.0.post12.dev5->tensorrt==9.2.0.post12.dev5->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cublas-cu12/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cufft-cu12/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-curand-cu12/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-nccl-cu12/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.1.0 (from torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.2.0a->tensorrt_llm==0.8.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->tensorrt_llm==0.8.0) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->tensorrt_llm==0.8.0) (2.0.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python->tensorrt_llm==0.8.0) (3.0.10)\n",
            "Collecting datasets>=2.0.0 (from evaluate->tensorrt_llm==0.8.0)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from evaluate->tensorrt_llm==0.8.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate->tensorrt_llm==0.8.0) (2.0.3)\n",
            "Collecting xxhash (from evaluate->tensorrt_llm==0.8.0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate->tensorrt_llm==0.8.0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from optimum->tensorrt_llm==0.8.0)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers[sentencepiece]<4.41.0,>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum->tensorrt_llm==0.8.0) (4.40.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (3.9.5)\n",
            "Collecting huggingface-hub (from accelerate==0.25.0->tensorrt_llm==0.8.0)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime~=1.16.1->nvidia-ammo~=0.7.0->tensorrt_llm==0.8.0) (24.3.25)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.15.0->tensorrt_llm==0.8.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.15.0->tensorrt_llm==0.8.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.15.0->tensorrt_llm==0.8.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.15.0->tensorrt_llm==0.8.0) (2024.2.2)\n",
            "INFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers[sentencepiece]<4.41.0,>=4.26.0 (from optimum->tensorrt_llm==0.8.0)\n",
            "  Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.1-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.0-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.1-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->optimum->tensorrt_llm==0.8.0)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.15.0->tensorrt_llm==0.8.0) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<=2.2.0a->tensorrt_llm==0.8.0) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->tensorrt_llm==0.8.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->tensorrt_llm==0.8.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate->tensorrt_llm==0.8.0) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<=2.2.0a->tensorrt_llm==0.8.0) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate->tensorrt_llm==0.8.0) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate->tensorrt_llm==0.8.0) (1.16.0)\n",
            "Building wheels for collected packages: tensorrt, mpi4py\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-9.2.0.post12.dev5-py2.py3-none-any.whl size=17625 sha256=0bbf4454a804bab90b7f48f8e2f4f07e854ad0e781d1a9c2633ae13b1550080f\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/96/bf/028c219d3560856a5fdb8b3aec8bf01e9d485521c092a64d02\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.6-cp310-cp310-linux_x86_64.whl size=2746303 sha256=b2aa5fc0eb4f7e63ca8ad634dbdbf724768206dcc0dff1f602fb22d8300a889f\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/ca/89/8fc1fb1c620afca13bb41c630b1f948bbf446e0aaa4b762e10\n",
            "Successfully built tensorrt mpi4py\n",
            "Installing collected packages: tensorrt_bindings, ninja, xxhash, triton, pynvml, polygraphy, onnx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mpi4py, lark, janus, humanfriendly, dill, colored, onnx-graphsurgeon, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, coloredlogs, tokenizers, tensorrt_libs, onnxruntime, nvidia-cusolver-cu12, diffusers, transformers, torch, tensorrt, datasets, nvidia-ammo, evaluate, accelerate, optimum, tensorrt_llm\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.40.2\n",
            "    Uninstalling transformers-4.40.2:\n",
            "      Successfully uninstalled transformers-4.40.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.25.0 colored-2.2.4 coloredlogs-15.0.1 datasets-2.19.1 diffusers-0.15.0 dill-0.3.8 evaluate-0.4.2 huggingface-hub-0.23.0 humanfriendly-10.0 janus-1.0.0 lark-1.1.9 mpi4py-3.1.6 multiprocess-0.70.16 ninja-1.11.1.1 nvidia-ammo-0.7.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 onnx-1.16.0 onnx-graphsurgeon-0.5.2 onnxruntime-1.16.3 optimum-1.19.2 polygraphy-0.49.9 pynvml-11.5.0 tensorrt-9.2.0.post12.dev5 tensorrt_bindings-9.2.0.post12.dev5 tensorrt_libs-9.2.0.post12.dev5 tensorrt_llm-0.8.0 tokenizers-0.15.2 torch-2.1.2 transformers-4.36.1 triton-2.1.0 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/TensorRT-LLM/examples/llama/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ouf0iVHDQjz",
        "outputId": "710bd069-0e3b-4968-a097-46f04c921a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.14.6 (from -r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1))\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/493.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m368.6/493.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate~=0.4.1 (from -r /content/TensorRT-LLM/examples/llama/requirements.txt (line 2))\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge_score~=0.1.2 (from -r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentencepiece~=0.1.99 in /usr/local/lib/python3.10/dist-packages (from -r /content/TensorRT-LLM/examples/llama/requirements.txt (line 4)) (0.1.99)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (14.0.2)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (4.64.1)\n",
            "Collecting xxhash (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1))\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score~=0.1.2->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score~=0.1.2->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3)) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score~=0.1.2->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2024.2.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score~=0.1.2->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score~=0.1.2->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score~=0.1.2->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 3)) (2023.12.25)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.6->-r /content/TensorRT-LLM/examples/llama/requirements.txt (line 1)) (2024.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=f7fa0d22c6f79dd2143531a195fbe8ae671a129bc4e8b28ac65e96235dbae590\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: xxhash, dill, rouge_score, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 evaluate-0.4.2 multiprocess-0.70.15 rouge_score-0.1.2 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorRT and its bindings\n",
        "!pip install --upgrade tensorrt\n",
        "!pip install tensorrt_bindings==8.6.1\n",
        "\n",
        "# Import TensorRT\n",
        "import tensorrt\n",
        "import sys\n",
        "\n",
        "# Add TensorRT module path (replace with actual path if necessary)\n",
        "# sys.path.append(\"/path/to/tensorrt/module\")\n",
        "\n",
        "# Print TensorRT version\n",
        "print(f\"TensorRT version: {tensorrt.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "a3yXhKB7p1Fe",
        "outputId": "90f2ec09-80a2-474b-f41a-f522440e5c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorrt in /usr/local/lib/python3.10/dist-packages (10.0.1)\n",
            "Requirement already satisfied: tensorrt-cu12 in /usr/local/lib/python3.10/dist-packages (from tensorrt) (10.0.1)\n",
            "Requirement already satisfied: tensorrt in /usr/local/lib/python3.10/dist-packages (10.0.1)\n",
            "Requirement already satisfied: tensorrt-cu12 in /usr/local/lib/python3.10/dist-packages (from tensorrt) (10.0.1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorrt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-7768e733ed96>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install --upgrade tensorrt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tensorrt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/tensorrt/module\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorrt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Llama 8B model using a single GPU and BF16.\n",
        "!python3 /content/TensorRT-LLM/examples/llama/convert_checkpoint.py --model_dir /content/Meta-Llama-3-8B-Instruct \\\n",
        "            --output_dir ./tllm_checkpoint_1gpu_bf16 \\\n",
        "            --load_model_on_cpu \\\n",
        "            --dtype bfloat16\n",
        "\n",
        "# !python convert_checkpoint.py --model_dir mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "#             --output_dir /output/mixtral-w4a16-tp2 \\\n",
        "#             --dtype bfloat16 \\\n",
        "#             --tp_size 2 \\\n",
        "#             --use_weight_only \\\n",
        "#             --weight_only_precision \\\n",
        "#             --int4 \\\n",
        "#             --moe_num_experts 8 \\\n",
        "#             --moe_top_k 2 \\\n",
        "#             --load_model_on_cpu\n",
        "\n",
        "!trtllm-build --checkpoint_dir ./tllm_checkpoint_1gpu_bf16 \\\n",
        "            --output_dir ./tmp/llama/8B/trt_engines/bf16/1-gpu \\\n",
        "            --gpt_attention_plugin bfloat16 \\\n",
        "            --gemm_plugin bfloat16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_kxyeM1IonM",
        "outputId": "bf360b64-cc12-4c27-88fa-43b5956e3366"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TensorRT-LLM] TensorRT-LLM version: 0.8.00.8.0\n",
            "Loading checkpoint shards: 100% 4/4 [00:01<00:00,  3.00it/s]\n",
            "Weights loaded. Total time: 00:00:53\n",
            "Total time of converting checkpoints: 00:02:03\n",
            "[TensorRT-LLM] TensorRT-LLM version: 0.8.0[05/16/2024-18:38:23] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set gpt_attention_plugin to bfloat16.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set gemm_plugin to bfloat16.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set lookup_plugin to None.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set lora_plugin to None.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set context_fmha to True.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set remove_input_padding to True.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set multi_block_mode to False.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set enable_xqa to True.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set tokens_per_block to 128.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.\n",
            "[05/16/2024-18:38:23] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. \n",
            "It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.\n",
            "[05/16/2024-18:38:23] [TRT] [I] [MemUsageChange] Init CUDA: CPU +12, GPU +0, now: CPU 257, GPU 308 (MiB)\n",
            "[05/16/2024-18:38:28] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +482, GPU +80, now: CPU 875, GPU 388 (MiB)\n",
            "[05/16/2024-18:38:28] [TRT-LLM] [I] Set nccl_plugin to None.\n",
            "[05/16/2024-18:38:28] [TRT-LLM] [I] Set use_custom_all_reduce to True.\n",
            "[05/16/2024-18:38:28] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.\n",
            "[05/16/2024-18:38:28] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.\n",
            "[TensorRT-LLM][ERROR] tensorrt_llm::common::TllmException: [TensorRT-LLM][ERROR] Assertion failed: Unsupported data type, pre SM 80 GPUs do not support bfloat16 (/home/jenkins/agent/workspace/LLM/release-0.8/L0_PostMerge/tensorrt_llm/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp:444)\n",
            "1       0x7c676981f803 /usr/local/lib/python3.10/dist-packages/tensorrt_llm/libs/libnvinfer_plugin_tensorrt_llm.so(+0x38803) [0x7c676981f803]\n",
            "2       0x7c676981fa9e /usr/local/lib/python3.10/dist-packages/tensorrt_llm/libs/libnvinfer_plugin_tensorrt_llm.so(+0x38a9e) [0x7c676981fa9e]\n",
            "3       0x7c67698470eb tensorrt_llm::plugins::GPTAttentionPlugin::GPTAttentionPlugin(int, int, int, int, float, tensorrt_llm::kernels::PositionEmbeddingType, int, float, tensorrt_llm::kernels::RotaryScalingType, float, int, int, int, bool, tensorrt_llm::kernels::ContextFMHAType, bool, bool, int, bool, tensorrt_llm::kernels::AttentionMaskType, bool, int, nvinfer1::DataType, int, bool, bool, int, bool, bool, bool, bool, bool) + 219\n",
            "4       0x7c6769847b84 tensorrt_llm::plugins::GPTAttentionPluginCreator::createPlugin(char const*, nvinfer1::PluginFieldCollection const*) + 2644\n",
            "5       0x7c67f416030a /usr/local/lib/python3.10/dist-packages/tensorrt_bindings/tensorrt.so(+0x16030a) [0x7c67f416030a]\n",
            "6       0x7c67f4043433 /usr/local/lib/python3.10/dist-packages/tensorrt_bindings/tensorrt.so(+0x43433) [0x7c67f4043433]\n",
            "7       0x5ab4c690210e /usr/bin/python3(+0x15a10e) [0x5ab4c690210e]\n",
            "8       0x5ab4c68f8a7b _PyObject_MakeTpCall + 603\n",
            "9       0x5ab4c6910acb /usr/bin/python3(+0x168acb) [0x5ab4c6910acb]\n",
            "10      0x5ab4c68f0cfa _PyEval_EvalFrameDefault + 24906\n",
            "11      0x5ab4c69029fc _PyFunction_Vectorcall + 124\n",
            "12      0x5ab4c6911492 PyObject_Call + 290\n",
            "13      0x5ab4c68ed5d7 _PyEval_EvalFrameDefault + 10791\n",
            "14      0x5ab4c69029fc _PyFunction_Vectorcall + 124\n",
            "15      0x5ab4c6911492 PyObject_Call + 290\n",
            "16      0x5ab4c68ed5d7 _PyEval_EvalFrameDefault + 10791\n",
            "17      0x5ab4c69107f1 /usr/bin/python3(+0x1687f1) [0x5ab4c69107f1]\n",
            "18      0x5ab4c6911492 PyObject_Call + 290\n",
            "19      0x5ab4c68ed5d7 _PyEval_EvalFrameDefault + 10791\n",
            "20      0x5ab4c69029fc _PyFunction_Vectorcall + 124\n",
            "21      0x5ab4c68f7cbd _PyObject_FastCallDictTstate + 365\n",
            "22      0x5ab4c690d86c _PyObject_Call_Prepend + 92\n",
            "23      0x5ab4c6a28700 /usr/bin/python3(+0x280700) [0x5ab4c6a28700]\n",
            "24      0x5ab4c68f8a7b _PyObject_MakeTpCall + 603\n",
            "25      0x5ab4c68f2150 _PyEval_EvalFrameDefault + 30112\n",
            "26      0x5ab4c69107f1 /usr/bin/python3(+0x1687f1) [0x5ab4c69107f1]\n",
            "27      0x5ab4c6911492 PyObject_Call + 290\n",
            "28      0x5ab4c68ed5d7 _PyEval_EvalFrameDefault + 10791\n",
            "29      0x5ab4c69029fc _PyFunction_Vectorcall + 124\n",
            "30      0x5ab4c68f7cbd _PyObject_FastCallDictTstate + 365\n",
            "31      0x5ab4c690d86c _PyObject_Call_Prepend + 92\n",
            "32      0x5ab4c6a28700 /usr/bin/python3(+0x280700) [0x5ab4c6a28700]\n",
            "33      0x5ab4c691142b PyObject_Call + 187\n",
            "34      0x5ab4c68ed5d7 _PyEval_EvalFrameDefault + 10791\n",
            "35      0x5ab4c69107f1 /usr/bin/python3(+0x1687f1) [0x5ab4c69107f1]\n",
            "36      0x5ab4c68ec53c _PyEval_EvalFrameDefault + 6540\n",
            "37      0x5ab4c69107f1 /usr/bin/python3(+0x1687f1) [0x5ab4c69107f1]\n",
            "38      0x5ab4c6911492 PyObject_Call + 290\n",
            "39      0x5ab4c68ed5d7 _PyEval_EvalFrameDefault + 10791\n",
            "40      0x5ab4c69107f1 /usr/bin/python3(+0x1687f1) [0x5ab4c69107f1]\n",
            "41      0x5ab4c6911492 PyObject_Call + 290\n",
            "42      0x5ab4c68ed5d7 _PyEval_EvalFrameDefault + 10791\n",
            "43      0x5ab4c69029fc _PyFunction_Vectorcall + 124\n",
            "44      0x5ab4c68f7cbd _PyObject_FastCallDictTstate + 365\n",
            "45      0x5ab4c690d86c _PyObject_Call_Prepend + 92\n",
            "46      0x5ab4c6a28700 /usr/bin/python3(+0x280700) [0x5ab4c6a28700]\n",
            "47      0x5ab4c691142b PyObject_Call + 187\n",
            "48      0x5ab4c68ed5d7 _PyEval_EvalFrameDefault + 10791\n",
            "49      0x5ab4c69029fc _PyFunction_Vectorcall + 124\n",
            "50      0x5ab4c68eb26d _PyEval_EvalFrameDefault + 1725\n",
            "51      0x5ab4c69029fc _PyFunction_Vectorcall + 124\n",
            "52      0x5ab4c6911492 PyObject_Call + 290\n",
            "53      0x5ab4c68ed5d7 _PyEval_EvalFrameDefault + 10791\n",
            "54      0x5ab4c69029fc _PyFunction_Vectorcall + 124\n",
            "55      0x5ab4c6911492 PyObject_Call + 290\n",
            "56      0x5ab4c68ed5d7 _PyEval_EvalFrameDefault + 10791\n",
            "57      0x5ab4c69029fc _PyFunction_Vectorcall + 124\n",
            "58      0x5ab4c6911492 PyObject_Call + 290\n",
            "59      0x5ab4c68ed5d7 _PyEval_EvalFrameDefault + 10791\n",
            "60      0x5ab4c69029fc _PyFunction_Vectorcall + 124\n",
            "61      0x5ab4c68eb26d _PyEval_EvalFrameDefault + 1725\n",
            "62      0x5ab4c68e79c6 /usr/bin/python3(+0x13f9c6) [0x5ab4c68e79c6]\n",
            "63      0x5ab4c69dd256 PyEval_EvalCode + 134\n",
            "64      0x5ab4c6a08108 /usr/bin/python3(+0x260108) [0x5ab4c6a08108]\n",
            "65      0x5ab4c6a019cb /usr/bin/python3(+0x2599cb) [0x5ab4c6a019cb]\n",
            "66      0x5ab4c6a07e55 /usr/bin/python3(+0x25fe55) [0x5ab4c6a07e55]\n",
            "67      0x5ab4c6a07338 _PyRun_SimpleFileObject + 424\n",
            "68      0x5ab4c6a06f83 _PyRun_AnyFileObject + 67\n",
            "69      0x5ab4c69f9a5e Py_RunMain + 702\n",
            "70      0x5ab4c69d002d Py_BytesMain + 45\n",
            "71      0x7c69646e7d90 /lib/x86_64-linux-gnu/libc.so.6(+0x29d90) [0x7c69646e7d90]\n",
            "72      0x7c69646e7e40 __libc_start_main + 128\n",
            "73      0x5ab4c69cff25 _start + 37\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/trtllm-build\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py\", line 497, in main\n",
            "    parallel_build(source, build_config, args.output_dir, workers,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py\", line 420, in parallel_build\n",
            "    passed = build_and_save(rank, rank % workers, ckpt_dir,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py\", line 392, in build_and_save\n",
            "    engine = build(build_config,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py\", line 282, in build\n",
            "    return build_model(model, build_config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py\", line 198, in build_model\n",
            "    model(**inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/module.py\", line 40, in __call__\n",
            "    output = self.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/modeling_utils.py\", line 498, in forward\n",
            "    hidden_states = self.transformer.forward(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/llama/model.py\", line 202, in forward\n",
            "    hidden_states = self.layers.forward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/modeling_utils.py\", line 255, in forward\n",
            "    hidden_states = layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/module.py\", line 40, in __call__\n",
            "    output = self.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/llama/model.py\", line 116, in forward\n",
            "    attention_output = self.attention(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/module.py\", line 40, in __call__\n",
            "    output = self.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/layers/attention.py\", line 742, in forward\n",
            "    context, past_key_value = gpt_attention(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/graph_rewriting.py\", line 561, in wrapper\n",
            "    outs = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorrt_llm/functional.py\", line 3813, in gpt_attention\n",
            "    layer = default_trtnet().add_plugin_v2(plug_inputs, attn_plug)\n",
            "TypeError: add_plugin_v2(): incompatible function arguments. The following argument types are supported:\n",
            "    1. (self: tensorrt_bindings.tensorrt.INetworkDefinition, inputs: List[tensorrt_bindings.tensorrt.ITensor], plugin: tensorrt_bindings.tensorrt.IPluginV2) -> tensorrt_bindings.tensorrt.IPluginV2Layer\n",
            "\n",
            "Invoked with: <tensorrt_bindings.tensorrt.INetworkDefinition object at 0x7c676babd130>, [<tensorrt_bindings.tensorrt.ITensor object at 0x7c67694957b0>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c6769466030>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c67694664b0>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c6769466bf0>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c676946f9f0>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c67694666b0>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c676946fbf0>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c67694662b0>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c67694440f0>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c6769444370>, <tensorrt_bindings.tensorrt.ITensor object at 0x7c67694668f0>], None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "Md-GfwtDlz1M",
        "outputId": "53d4a672-e5d1-464f-de71-c9c93c4610b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorrt in /usr/local/lib/python3.10/dist-packages (10.0.1)\n",
            "Requirement already satisfied: tensorrt-cu12 in /usr/local/lib/python3.10/dist-packages (from tensorrt) (10.0.1)\n",
            "Requirement already satisfied: tensorrt in /usr/local/lib/python3.10/dist-packages (10.0.1)\n",
            "Requirement already satisfied: tensorrt-cu12 in /usr/local/lib/python3.10/dist-packages (from tensorrt) (10.0.1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorrt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-3ec9f551634b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install --upgrade tensorrt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tensorrt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorrt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python convert_checkpoint.py --model_dir mistralai/Mixtral-8x7B-Instruct-v0.1 --output_dir /output/mixtral-w4a16-tp2/ --dtype bfloat16 --tp_size 2 --use_weight_only --weight_only_precision int4 --moe_num_experts 8 --moe_top_k 2 --load_model_on_cpu"
      ],
      "metadata": {
        "id": "0BQCxM38P8qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ND1X-A8Vg0Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}